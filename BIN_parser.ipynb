{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5614cba0",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0746d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Imports\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af04dd80",
   "metadata": {},
   "source": [
    "# Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6b7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_error_corpus_data = pd.read_csv(\"./generated_datasets/labeledData.csv\", encoding='latin-1')\n",
    "bin_data = pd.read_csv(\"./original_datasets/Storasnid_beygm.csv\", low_memory=False, encoding='latin-1', header=None, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e755a70",
   "metadata": {},
   "source": [
    "## Inspect the bin_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad246ed5",
   "metadata": {},
   "source": [
    "### Manipulate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused columns\n",
    "case_data = bin_data.drop([0,2,5,6,7], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0ba829",
   "metadata": {},
   "source": [
    "### Inspect our dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6381727",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a7a85c",
   "metadata": {},
   "source": [
    "## Inspect the ice_error_corpus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb8baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_error_corpus_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_error_corpus_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4163c79",
   "metadata": {},
   "source": [
    "### Manipulate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a4253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the correct sentences\n",
    "correct_sentences = ice_error_corpus_data.loc[ice_error_corpus_data['label'] == 'correct']\n",
    "correct_sentences.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9ef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneeded columns\n",
    "correct_sentences = correct_sentences.drop('label', axis=1)\n",
    "correct_sentences = correct_sentences.drop('Error', axis=1)\n",
    "correct_sentences.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows\n",
    "correct_sentences = correct_sentences.drop_duplicates()\n",
    "correct_sentences.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c85de",
   "metadata": {},
   "source": [
    "## Create a Validation Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small test set\n",
    "correct_sentences, test_set = train_test_split(correct_sentences, test_size=0.1)\n",
    "test_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137572ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the incorrect sentences\n",
    "incorrect_sentences = ice_error_corpus_data.loc[ice_error_corpus_data['label'] == 'incorrect']\n",
    "incorrect_sentences = incorrect_sentences[incorrect_sentences['Error'].str.contains('inflection')]\n",
    "incorrect_sentences = incorrect_sentences.drop('Error', axis=1)\n",
    "incorrect_sentences = incorrect_sentences.drop_duplicates()\n",
    "incorrect_sentences.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c624af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.concat([test_set,incorrect_sentences])\n",
    "test_set.fillna('correct', inplace=True)\n",
    "test_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7982b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8097ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.to_csv('synthetic_validation_set.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1bbd7e",
   "metadata": {},
   "source": [
    "# Load helping functions meant to be abstracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca3ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that, if given a correct sentence, returns an array of incorrectly declined sentences\n",
    "def generate_incorrect_sentences(correct_sentence):\n",
    "    # An array to gather incorrect sentences\n",
    "    incorrect_sentences = []\n",
    "    # Split the correct sentence into a word array\n",
    "    word_array = correct_sentence.split()\n",
    "    # For each word in the array, \n",
    "    for x in range(len(word_array)):\n",
    "        # Find the word in the Dataframe\n",
    "        df_same_words = case_data.loc[case_data[3] == word_array[x]]\n",
    "        if not(df_same_words.empty):\n",
    "            # Sample a new cases of that word\n",
    "            new_cases = sample_new_cases(df_same_words)\n",
    "            # If a new case was found, make a new sentence and add it to the array\n",
    "            for new_case in new_cases:\n",
    "                incorrect_string = \"\"\n",
    "                for y in range(len(word_array)):\n",
    "                    if not y == x:\n",
    "                        incorrect_string += word_array[y] + \" \"\n",
    "                    else:\n",
    "                        incorrect_string += new_case + \" \"\n",
    "                incorrect_string = incorrect_string[:len(incorrect_string)-1]\n",
    "                incorrect_sentences.append(incorrect_string)\n",
    "    return incorrect_sentences\n",
    "\n",
    "# A function that returns an array of different declensions of a word\n",
    "def sample_new_cases(df_same_words):\n",
    "    output_array = []\n",
    "    array_of_relevant_cases = return_array_of_incorrect_case_types(df_same_words)\n",
    "    all_word_ids = set(df_same_words[1].tolist())\n",
    "    for id_ in all_word_ids:\n",
    "        df_other_cases = case_data.loc[case_data[1] == id_]\n",
    "        df_relevant_cases =  df_other_cases[3][df_other_cases[4].isin(array_of_relevant_cases)]\n",
    "        for word in df_relevant_cases:\n",
    "            output_array.append(word)\n",
    "    return output_array\n",
    "\n",
    "def return_array_of_incorrect_case_types(df_same_words):\n",
    "    array_of_correct_cases = []\n",
    "    array_of_incorrect_cases = []\n",
    "    for case in df_same_words[4]:\n",
    "        array_of_correct_cases.append(case)\n",
    "        # IF It's a question (SP)\n",
    "        if case[0:2] == \"SP\":\n",
    "            index = case.find(\"VH\")\n",
    "            if index == -1:\n",
    "                index = case.find(\"FH\")\n",
    "                newString = case[0:index]+\"VH\"+case[index+2:]\n",
    "            else:\n",
    "                newString = case[0:index]+\"FH\"+case[index+2:]\n",
    "        # IF It's a non Noun (GM-OP-MM)\n",
    "        elif case[0:2] == \"GM\" or  case[0:2] == \"OP\" or case[0:2] == \"MM\":\n",
    "            skip = False\n",
    "            index = case.find(\"1P\")\n",
    "            if not index == -1:\n",
    "                newString1 = case[0:index]+\"2P\"+case[index+2:]\n",
    "                newString2 = case[0:index]+\"3P\"+case[index+2:]\n",
    "            else:\n",
    "                index = case.find(\"2P\")\n",
    "                if not index == -1:\n",
    "                    newString1 = case[0:index]+\"1P\"+case[index+2:]\n",
    "                    newString2 = case[0:index]+\"3P\"+case[index+2:]\n",
    "                else:\n",
    "                    index = case.find(\"3P\")\n",
    "                    if not index == -1:\n",
    "                        newString1 = case[0:index]+\"1P\"+case[index+2:]\n",
    "                        newString2 = case[0:index]+\"2P\"+case[index+2:]\n",
    "                    else:\n",
    "                        skip = True\n",
    "            if not skip:\n",
    "                array_of_incorrect_cases.append(newString1)\n",
    "                array_of_incorrect_cases.append(newString2)\n",
    "        # If it's a Noun (NF,EF,ÞGF,ÞF)\n",
    "        else:\n",
    "            index = case.find(\"NF\")\n",
    "            if not index ==-1:\n",
    "                newString1 = case[0:index]+\"EF\"+case[index+2:]\n",
    "                newString2 = case[0:index]+\"ÞGF\"+case[index+2:]\n",
    "                newString3 = case[0:index]+\"ÞF\"+case[index+2:]\n",
    "            if index == -1:\n",
    "                index = case.find(\"EF\")\n",
    "                if not index == -1:\n",
    "                    newString1 = case[0:index]+\"NF\"+case[index+2:]\n",
    "                    newString2 = case[0:index]+\"ÞGF\"+case[index+2:]\n",
    "                    newString3 = case[0:index]+\"ÞF\"+case[index+2:]\n",
    "            if index == -1:\n",
    "                index = case.find(\"ÞGF\")\n",
    "                if not index == -1:\n",
    "                    newString1 = case[0:index]+\"EF\"+case[index+3:]\n",
    "                    newString2 = case[0:index]+\"NF\"+case[index+3:]\n",
    "                    newString3 = case[0:index]+\"ÞF\"+case[index+3:]\n",
    "            if index == -1:\n",
    "                index = case.find(\"ÞF\")\n",
    "                if not index == -1:\n",
    "                    newString1 = case[0:index]+\"EF\"+case[index+2:]\n",
    "                    newString2 = case[0:index]+\"ÞGF\"+case[index+2:]\n",
    "                    newString3 = case[0:index]+\"NF\"+case[index+2:]\n",
    "            if not index == -1:\n",
    "                array_of_incorrect_cases.append(newString1)\n",
    "                array_of_incorrect_cases.append(newString2)\n",
    "                array_of_incorrect_cases.append(newString3)\n",
    "    output_array = [x for x in array_of_incorrect_cases if x not in array_of_correct_cases]\n",
    "    output_array = set(output_array)\n",
    "    return output_array\n",
    "                \n",
    "\n",
    "# A function that returns an array, with one random column from each row\n",
    "def sample_column(csv):\n",
    "    data = pd.read_csv('./generated_datasets/synthetic_data.csv', encoding='utf-8')\n",
    "    data = data.dropna(axis = 0, how = 'all')\n",
    "    array = []\n",
    "    for i in range(len(data)):\n",
    "        number_of_columns = (data.iloc[i].notnull().sum())\n",
    "        rng_column = np.random.randint(0,number_of_columns)\n",
    "        array.append([data.iloc[i][rng_column],\"incorrect\"])\n",
    "    return (array)       \n",
    "            \n",
    "def create_csv(data):\n",
    "    with open('synthetic_data.csv', 'w', newline='', encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"text\",\"label\"])\n",
    "        for row in data:\n",
    "            for column in row:\n",
    "                writer.writerow([column,\"incorrect\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d118c30",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75abf7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Array that collects our incorrect sentences\n",
    "output = []\n",
    "\n",
    "# Variable used to track the progress of the parser\n",
    "rows_to_use = len(correct_sentences)\n",
    "milestone = rows_to_use/10\n",
    "    \n",
    "for i in range(rows_to_use):\n",
    "    correct_sentence = correct_sentences['text'].iloc[i]\n",
    "    incorrect_sentences = generate_incorrect_sentences(correct_sentence)\n",
    "    output.append(incorrect_sentences)\n",
    "    \n",
    "    # Track the progress of the parser\n",
    "    if(i>=milestone):\n",
    "        print(\"Finished : \",(milestone/(rows_to_use/10))*10,\"%\")\n",
    "        milestone += rows_to_use/10\n",
    "print(\"Finished!\")\n",
    "    \n",
    "# Create a backup of the object to work with, just in case\n",
    "my_back_up = output.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df2741",
   "metadata": {},
   "source": [
    "# my_data = output.copy()\n",
    "full_CSV = create_csv(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "# Open the CSV file and read the rows\n",
    "with open('./generated_datasets/April/synthetic_validation_set.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    rows = list(csv_reader)\n",
    "\n",
    "# Shuffle the rows randomly\n",
    "random.shuffle(rows)\n",
    "\n",
    "# Write the shuffled rows to a new CSV file\n",
    "with open('shuffled_validation_file.csv', 'w', newline='') as shuffled_file:\n",
    "    csv_writer = csv.writer(shuffled_file)\n",
    "    csv_writer.writerows(rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
