{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c0c495",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ee651a",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0458decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "!pip install transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "!pip install datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27701b90",
   "metadata": {},
   "source": [
    "## Load our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c709a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our two datasets\n",
    "# 1. IceErrorCorpus (IEC)\n",
    "correct_data = pd.read_csv(\"./generated_datasets/labeled_data.csv\", encoding='latin-1')\n",
    "correct_data = correct_data.loc[correct_data['label'] == 'correct']\n",
    "correct_data = correct_data.drop(['Error'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7867c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Our synthetic dataset generated from IEC and BIN\n",
    "error_data = pd.read_csv(\"./generated_datasets/synthetic_data.csv\", encoding='UTF-8')\n",
    "error_data = error_data.drop(['type'], axis=1)\n",
    "error_data = error_data.drop(['error position'], axis=1)\n",
    "error_data  = error_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the correct and synthetic datasets\n",
    "data = pd.concat([correct_data, error_data])\n",
    "# Replace  the labels with numeric values\n",
    "data['label'] = data['label'].replace(['correct','incorrect'],[0,1]) \n",
    "# Split the data into training and testing sets\n",
    "train, test = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d0667",
   "metadata": {},
   "source": [
    "### Explore our Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22d8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefb21a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ce9d4",
   "metadata": {},
   "source": [
    "### Create a Dataset dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef069cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset.from_pandas(train)\n",
    "test = Dataset.from_pandas(test)\n",
    " \n",
    "dataset = DatasetDict()\n",
    " \n",
    "dataset['train'] = train\n",
    "dataset['test'] = test\n",
    "\n",
    "dataset = dataset.remove_columns(\"__index_level_0__\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06617bc3",
   "metadata": {},
   "source": [
    "## Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e705d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"correct\", 1: \"incorrect\"}\n",
    "label2id = {\"incorrect\": 0, \"correct\": 1}\n",
    "\n",
    "# Model Architecture\n",
    "myTokenizer = AutoTokenizer.from_pretrained(\"mideind/IceBERT\")\n",
    "myModel = AutoModelForSequenceClassification.from_pretrained(\"mideind/IceBERT\", num_labels=2, id2label=id2label, label2id=label2id)\n",
    "classifier = pipeline(\"text-classification\", model=myModel, tokenizer=myTokenizer)\n",
    "\n",
    "# Tokenizer\n",
    "def preprocess_function(examples):\n",
    "    return myTokenizer(examples[\"text\"], truncation=True)\n",
    "dataset = dataset.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=myTokenizer)\n",
    "\n",
    "# Training Args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"trainingArguments\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=myModel,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=myTokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae4c5fb",
   "metadata": {},
   "source": [
    "# Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915474cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"refactored_model_v2_non_nouns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d1310",
   "metadata": {},
   "source": [
    "# Sanity check our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07fad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"text-classification\", model=myModel, tokenizer=myTokenizer)\n",
    "\n",
    "# I went to \"Torfi's\" last night and had a beer.\n",
    "test1 = classifier(\"Ég fór til Torfi í gær og fékk mér bjór.\")\n",
    "test2 = classifier(\"Ég fór til Torfa í gær og fékk mér bjór.\")\n",
    "print(test1)\n",
    "print(test2)\n",
    "print(\"---\")\n",
    "# The stable is painted blue.\n",
    "res1 = classifier(\"Hestahúsið er málaður blátt.\")\n",
    "res2 = classifier(\"Hestahúsið er málað blátt.\")\n",
    "print(res1)\n",
    "print(res2)\n",
    "print(\"---\")\n",
    "# My captain can't drive very well, but she owns a red dog.\n",
    "res1 = classifier(\"Skipstjórinn minn kann ekki að keyra mjög vel en hún á rauður hund.\")\n",
    "res2 = classifier(\"Skipstjórinn minn kann ekki að keyra mjög vel en hún á rauðan hund.\")\n",
    "print(res1)\n",
    "print(res2)\n",
    "print(\"---\")\n",
    "# I drive my car every day.\n",
    "res1 = classifier(\"Ég keyri bíllinn minn á hverjum einasta degi.\")\n",
    "res2 = classifier(\"Ég keyri bílinn minn á hverjum einasta degi.\")\n",
    "print(res1)\n",
    "print(res2)\n",
    "print(\"---\")\n",
    "# I drive my car every day\n",
    "res1 = classifier(\"Ég keyri bílinn mínum á hverjum einasta degi.\")\n",
    "res2 = classifier(\"Ég keyri bílinn minn á hverjum einasta degi.\")\n",
    "print(res1)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3866921f",
   "metadata": {},
   "source": [
    "# Evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4072f684",
   "metadata": {},
   "outputs": [],
   "source": [
    "truePos = []\n",
    "trueNeg = []\n",
    "falsePos = []\n",
    "falseNeg = []\n",
    "\n",
    "# Open the csv file, and read each sentence\n",
    "with open('./generated_datasets/April/synthetic_validation_set.csv') as csv_file:\n",
    "    my_file = csv.reader(csv_file)\n",
    "    next(my_file, None)  # skip the headers\n",
    "    for line in my_file:\n",
    "        prediction = classifier(line[0])\n",
    "        if prediction[0]['label'] == 'correct':\n",
    "            if line[1] == 'correct':\n",
    "                truePos.append(prediction)\n",
    "            elif line[1] == 'incorrect':\n",
    "                falsePos.append(prediction)\n",
    "            else:\n",
    "                print(\"Unexpected Error code 001\")\n",
    "        elif prediction[0]['label'] == 'incorrect':\n",
    "            if line[1] == 'incorrect':\n",
    "                 trueNeg.append(prediction)\n",
    "            elif line[1] == 'correct':\n",
    "                 falseNeg.append(prediction)\n",
    "            else:\n",
    "                print(\"Unexpected Error code 002\")\n",
    "        else:\n",
    "            print(\"Unexpected Error code 003\")\n",
    "    \n",
    "print(\"True Positives:  \",len(truePos))\n",
    "print(\"True Negatives:  \",len(trueNeg))\n",
    "print(\"False Positives: \",len(falsePos))\n",
    "print(\"False Negatives: \",len(falseNeg))\n",
    "\n",
    "accuracy = (len(truePos)+len(trueNeg))/(len(truePos)+len(trueNeg)+len(falsePos)+len(falseNeg))\n",
    "precision = (len(truePos))/(len(truePos)+len(falsePos))\n",
    "recall = (len(truePos))/(len(truePos)+len(falseNeg))\n",
    "f1 = 2* ((precision*recall)/(precision+recall))\n",
    "\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F1-score: \",f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
